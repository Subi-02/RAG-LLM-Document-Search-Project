# RAG-LLM-Document-Search-Project
Overview

This project demonstrates a Retrieval-Augmented Generation (RAG) system that allows semantic search over documents using a Large Language Model (LLM).

Key Idea:
Instead of feeding the entire document collection to an LLM, the system first retrieves relevant chunks using embeddings and a vector database (FAISS), then generates answers only from the retrieved context. This improves efficiency, accuracy, and scalability.

**Applications:**

Enterprise knowledge management

AI-driven virtual assistants

Back-office automation with NLP

Document question answering

**Features:**

✅ Ingest PDF and DOCX documents

✅ Split documents into smaller, overlapping chunks

✅ Generate embeddings using Sentence Transformers

✅ Store embeddings in a FAISS vector store for fast retrieval

✅ Query documents interactively via RAG + LLM

✅ Context-aware answer generation using local or cloud-based LLMs

**Architecture:**

Document Sources (PDF/DOCX)
        │
        ▼
   Document Loader
        │
        ▼
   Text Chunking
        │
        ▼
 Embeddings Generation (SentenceTransformer)
        │
        ▼
    FAISS Vector Store
        │
        ▼
User Query ──► Query Embedding ──► Top-k Retrieval ──► Context
        │
        ▼
     LLM (Ollama / OpenAI)
        │
        ▼
   Generated Answer

**Project Structure:**

rag/
├── docs/                  # Folder containing PDFs/DOCX documents
├── ingest.py              # Document ingestion & embedding creation
├── vector.index           # FAISS index generated by ingest.py
├── chunks.pkl             # Serialized document chunks
├── rag_llm.py             # Query interface using RAG + LLM
├── requirements.txt       # Python dependencies
└── README.md              # Project documentation


Setup Instructions:

1️⃣ Clone the repository
git clone <your-repo-url>
cd rag

2️⃣ Create a virtual environment
python -m venv rag_env
rag_env\Scripts\activate    # Windows
source rag_env/bin/activate # Linux / Mac

3️⃣ Install dependencies
pip install --upgrade pip
pip install -r requirements.txt


Manual installation (if needed):

pip install langchain openai faiss-cpu python-docx PyPDF2 tiktoken sentence-transformers ollama

4️⃣ Set OpenAI API Key (optional, required for OpenAI embeddings/LLM)
setx OPENAI_API_KEY "your_openai_api_key_here"  # Windows
export OPENAI_API_KEY="your_openai_api_key_here" # Linux / Mac

5️⃣ Add Documents

Place your PDF or DOCX files in the docs/ folder.
Example: docs/alice_in_wonderland.pdf.

6️⃣ Run Ingestion Script
python ingest.py


What this does:

Loads all documents from docs/

Splits text into smaller chunks

Generates embeddings for each chunk

Saves FAISS index (vector.index) and document chunks (chunks.pkl)

7️⃣ Query Documents
python rag_llm.py


Example Query:

Your question: Who is Alice in Alice in Wonderland?


**Workflow:**

Embed the query using SentenceTransformer

Retrieve top-k most relevant document chunks from FAISS

Send the context and query to the LLM

Generate and display the answer

Type exit to quit.

**Dependencies:**

Python 3.12+

FAISS
 – Vector similarity search
 
LangChain
 – LLM orchestration
 
OpenAI API
 – Optional LLM/embedding
 
Sentence Transformers
 – Embedding generation
 
Ollama
 – Local LLM integration (optional)

Additional packages: PyPDF2, python-docx, tiktoken, numpy, pickle

**Future Enhancements:**

Support more document formats (TXT, HTML, Markdown)

Batch query processing

Web interface with Flask / FastAPI

Online updates to FAISS index for new documents

Domain-specific embeddings fine-tuning

Multi-language support for documents and queries
